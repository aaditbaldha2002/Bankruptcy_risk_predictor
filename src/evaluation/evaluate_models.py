import logging
from typing import List
from mlflow.tracking import MlflowClient

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


def extract_metrics_from_model_uri(model_uri: str) -> dict:
    """
    Extracts metrics from an MLflow run using the model URI.
    Args:
        model_uri (str): e.g., runs:/<run_id>/model_artifact_path

    Returns:
        dict: Metrics dictionary
    """
    run_id = model_uri.split("/")[1]
    client = MlflowClient()
    run_data = client.get_run(run_id).data
    return run_data.metrics


def evaluate_models(
    test_data_path: str,
    classifier_model_uri: str,
    regressor_model_uris: List[str],
    classifier_threshold: float = 0.85,
    regressor_rmse_threshold: float = 0.03,
    regressor_r2_threshold: float = 0.8
) -> bool:
    logger.info("ğŸ” Starting evaluation using model URIs...")

    # -----------------------
    # Evaluate Classification
    # -----------------------
    try:
        logger.info(f"ğŸ“¦ Loading classifier metrics from: {classifier_model_uri}")
        clf_metrics = extract_metrics_from_model_uri(classifier_model_uri)
        clf_accuracy = float(clf_metrics.get("accuracy", 0.0))
        clf_f1 = float(clf_metrics.get("f1_score", 0.0))

        logger.info(f"ğŸ“Š Classifier Accuracy: {clf_accuracy}, F1 Score: {clf_f1}")

        if clf_accuracy < classifier_threshold:
            logger.warning("âŒ Classifier failed threshold.")
            return False
        logger.info("âœ… Classifier passed.")
    except Exception as e:
        logger.exception("âŒ Failed to evaluate classifier.")
        return False

    # ---------------------
    # Evaluate Regressors
    # ---------------------
    passed_regressors = 0
    for idx, uri in enumerate(regressor_model_uris):
        try:
            logger.info(f"ğŸ“¦ Loading regressor metrics from: {uri}")
            reg_metrics = extract_metrics_from_model_uri(uri)

            rmse = float(reg_metrics.get("rmse", 1e9))  # Default to large number
            r2 = float(reg_metrics.get("r2_score", 0.0))

            logger.info(f"ğŸ“Š Regressor[{idx}] RMSE: {rmse}, RÂ²: {r2}")

            if rmse <= regressor_rmse_threshold and r2 >= regressor_r2_threshold:
                passed_regressors += 1
                logger.info(f"âœ… Regressor[{idx}] passed.")
            else:
                logger.warning(f"âŒ Regressor[{idx}] failed.")
        except Exception as e:
            logger.exception(f"âŒ Failed to evaluate Regressor[{idx}].")
            continue

    if passed_regressors == len(regressor_model_uris):
        logger.info("ğŸš€ All regressors passed. Deployment approved.")
        return True
    else:
        logger.warning(f"â›” Only {passed_regressors}/{len(regressor_model_uris)} regressors passed.")
        return False
